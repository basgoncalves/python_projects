{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory and years to use\n",
    "cwd = os.getcwd() # change for '__file__' if using .py \n",
    "years = list(range(1991,2023))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (skip if it has been done)\n",
    "url_general = \"https://www.basketball-reference.com/awards/awards_{}.html\"\n",
    "\n",
    "for year in years:\n",
    "    url = url_general.format(year)\n",
    "    data = requests.get(url)\n",
    "\n",
    "    filename = os.path.join(cwd, '.\\\\results\\\\{}.html'.format(year))\n",
    "    # filename = os.path.join('results\\{}.html'.format(year))\n",
    "    with open(filename,'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(data.text)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame for MVPs\n",
    "dfs =[]\n",
    "mvp_folder = (cwd + '\\\\mvps')\n",
    "if os.path.isdir(mvp_folder):\n",
    "     []\n",
    "else:\n",
    "     os.mkdir(mvp_folder)\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    filename = (mvp_folder + '\\\\{}.html'.format(year))\n",
    "    with open(filename,'r',encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        soup.find('tr', class_='over_header').decompose()\n",
    "        mvp_table = soup.find_all(id='all_mvp')\n",
    "        mvp_season = pd.read_html(str(mvp_table))[0]    # without the [0] mvp_season is a list and not table \n",
    "        mvp_season['Year'] = year                       # add a column called year\n",
    "\n",
    "        dfs.append(mvp_season)  \n",
    "\n",
    "mvps = pd.concat(dfs)\n",
    "\n",
    "filename = (cwd + '\\\\mvps.csv')\n",
    "mvps.to_csv(filename)\n",
    "mvps.head(3) #first N rows\n",
    "mvps.tail(2)  #last N rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup url and folders for ALL players\n",
    "player_stats_url = 'https://www.basketball-reference.com/leagues/NBA_{}_per_game.html'\n",
    "player_folder = (cwd + '\\\\players')\n",
    "if os.path.isdir(player_folder):\n",
    "     []\n",
    "else:\n",
    "     os.mkdir(player_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data from URL and save\n",
    "\n",
    "# this wont work because of a java issue in the website \n",
    "# https://www.youtube.com/watch?v=JGQGd-oa0l4&ab_channel=Dataquest\n",
    "\n",
    "for year in years:\n",
    "    url = player_stats_url.format(year)\n",
    "    data = requests.get(url)\n",
    "\n",
    "    filename = (player_folder + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(data.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the driver \n",
    "option = webdriver.ChromeOptions()   \n",
    "option.add_argument('headless')         # use this option not to open the browser (https://www.codegrepper.com/code-examples/python/selenium+without+opening+browser+python)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html data in player folder\n",
    "for year in years:\n",
    "    url = player_stats_url.format(year)\n",
    "    print(url)\n",
    "    data = driver.get(url)\n",
    "    driver.execute_script('window.scrollTo(1,10000)')\n",
    "    time.sleep(2)\n",
    "\n",
    "    html = driver.page_source\n",
    "    filename = (player_folder + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "print('finished scrapping!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished! Data saved in \"player_stats.csv\"\n"
     ]
    }
   ],
   "source": [
    "# create data frame for All players\n",
    "dfs =[]\n",
    "for year in years:\n",
    "\n",
    "    filename = (player_folder + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'r',encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        soup.find('tr', class_='thead').decompose()\n",
    "        player_table = soup.find_all(id='all_per_game_stats')\n",
    "        player_season = pd.read_html(str(player_table))[0]      # without the [0] player_season is a list and not table \n",
    "        player_season['Year'] = year                            # add a column called year\n",
    "\n",
    "        dfs.append(player_season)  \n",
    "\n",
    "players = pd.concat(dfs)\n",
    "filename = (cwd + '\\\\player_stats.csv')\n",
    "players.to_csv(filename)\n",
    "print('finished! Data saved in \"player_stats.csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup url and folders for STANDINGS\n",
    "team_stats_url = 'https://www.basketball-reference.com/leagues/NBA_{}_standings.html'\n",
    "team_folder = (cwd + '\\\\team')\n",
    "if os.path.isdir(team_folder):\n",
    "     []\n",
    "else:\n",
    "     os.mkdir(team_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    url = team_stats_url.format(year)\n",
    "    data = requests.get(url)\n",
    "\n",
    "    filename = (team_folder + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished! Data saved as \"team_stats.csv\" \n"
     ]
    }
   ],
   "source": [
    "# create data frame for All teams\n",
    "dfs =[]\n",
    "for year in years:\n",
    "\n",
    "    filename = (team_folder + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'r',encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        soup.find('tr', class_='thead').decompose()\n",
    "\n",
    "        east_table = soup.find_all(id='all_divs_standings_E')\n",
    "        east_season = pd.read_html(str(east_table))[0]              # without the [0] this is a list and not table \n",
    "        east_season['Year'] = year                                  # add year column\n",
    "        east_season['Team'] = east_season[\"Eastern Conference\"]     # add column \"Team\" wich is originally called \"Eastern Conference\"\n",
    "        del east_season['Eastern Conference']\n",
    "\n",
    "        dfs.append(east_season)\n",
    "        \n",
    "        west_table = soup.find_all(id='all_divs_standings_W')   # do the same for west\n",
    "        west_season = pd.read_html(str(west_table))[0]      \n",
    "        west_season['Year'] = year                          \n",
    "        west_season['Team'] = west_season[\"Western Conference\"] \n",
    "        del west_season['Western Conference']\n",
    "\n",
    "        dfs.append(west_season)\n",
    "          \n",
    "\n",
    "teams = pd.concat(dfs)\n",
    "filename = (cwd + '\\\\team_stats.csv')\n",
    "teams.to_csv(filename)\n",
    "print('finished! Data saved as \"team_stats.csv\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup url and folders for STANDINGS\n",
    "url_2k = 'https://hoopshype.com/nba2k/{}'\n",
    "folder_2k = (cwd + '\\\\2k')\n",
    "if os.path.isdir(folder_2k):\n",
    "     []\n",
    "else:\n",
    "     os.mkdir(folder_2k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1999,2022))\n",
    "for year in years:\n",
    "    url = url_2k.format((str(year) + '-' + str(year + 1)))\n",
    "    data = requests.get(url)\n",
    "\n",
    "    filename = (folder_2k + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1999-2000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data frame for all 2k players\n",
    "years = list(range(1999,2022))\n",
    "dfs =[]\n",
    "for year in years:\n",
    "\n",
    "    filename = (folder_2k + '.\\\\{}.html'.format(year))\n",
    "    with open(filename,'r',encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        soup.find('tr', class_='thead').decompose()\n",
    "\n",
    "        east_table = soup.find_all(id='all_divs_standings_E')\n",
    "        east_season = pd.read_html(str(east_table))[0]              # without the [0] this is a list and not table \n",
    "        east_season['Year'] = year                                  # add year column\n",
    "        east_season['Team'] = east_season[\"Eastern Conference\"]     # add column \"Team\" wich is originally called \"Eastern Conference\"\n",
    "        del east_season['Eastern Conference']\n",
    "\n",
    "        dfs.append(east_season)\n",
    "        \n",
    "        west_table = soup.find_all(id='all_divs_standings_W')   # do the same for west\n",
    "        west_season = pd.read_html(str(west_table))[0]      \n",
    "        west_season['Year'] = year                          \n",
    "        west_season['Team'] = west_season[\"Western Conference\"] \n",
    "        del west_season['Western Conference']\n",
    "\n",
    "        dfs.append(west_season)\n",
    "          \n",
    "\n",
    "teams = pd.concat(dfs)\n",
    "filename = (cwd + '\\\\team_stats.csv')\n",
    "teams.to_csv(filename)\n",
    "print('finished! Data saved as \"team_stats.csv\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccaf4f2b870b44d404ebc97758e8555864a76105cb721a293c812313f4b24d8e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
